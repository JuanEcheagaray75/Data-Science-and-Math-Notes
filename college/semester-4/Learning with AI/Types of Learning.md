# Types of Learning

---

## Supervised

> The agent observes some example input-output pairs and learns a function that maps from input to output

The task of supervised learning can be defined as:

Given a _training set_ of $N$ example input-output pairs

$$(x_1, y_1), (x_2, y_2), \dots (x_N, y_N)$$

Where each $y_j$ was generated by an unknown function $y = f(x)$, discover a function $h$ that approximates the true function $f$.

Notice how there was no restriction on the values for $x$ and $y$, they need not be numbers. The function $h$ is an hypothesis. Learning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the _training set_. (I will not go over the difference and purpose of the _train_ and _test_ data sets, I've already covered them in great depth on previous courses).

- We say that a model _generalizes_ well if it correctly predicts the value of $y$ for unseen examples
- Sometimes the function $f$ is _stochastic_ not strictly a function of $x$, and what the agent must learn is a conditional probability distribution, $\mathbf{P}(Y|x)$
- When the output is one of a finite set of values, the learning problem is called **classification**
- When $y$ is a continuous value, the learning problem is called **regression**

We don't know what $f$ is, but we will approximate it with a function $h$ selected from an **hypothesis space**, $\mathbf{H}$. A function $h$ is said to be **consistent** if it agrees with all the data, but then, how shall we choose from among multiple consistent hypotheses? The general answer is to prioritize the **simplest model**.

> In general, there is a tradeoff between complex hypotheses that fit the training data well and simpler hypotheses that _may_ generalize better

We say that a learning problem is **realizable** if the hypothesis space contains the true function $f$, good luck finding out if your space contains $f$. Now you might be thinking, well hell, why not letting $\mathbf{H}$ be the set of all _Turing Machines_ or C++ programs? Think about the computational complexity of such endeavor.

> There is a tradeoff between the expressiveness of a hypothesis space and the complexity of finding a good hypothesis within that space 

---

## Unsupervised

> The agent learns patterns in the input even though no explicit feedback is supplied

## Reinforcement

 > The agent learns from a series of reinforcements-rewards or punishments